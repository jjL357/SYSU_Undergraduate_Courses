# coding=gb2312
import gzip
import os
import torch
import numpy as np
from PIL import Image
from matplotlib import pyplot as plt
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F
from torch import nn, optim
import random
import matplotlib.pyplot as plt

class CNN(nn.Module):#??????
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1,10,5)#???1:?????1??????10???????5
        self.linear1 = nn.Linear(1440,100)#???1:?????1440??????100
        self.linear2 = nn.Linear(100, 10)#???2:?????100??????10(10??)
    def forward(self,input):
        input_size = input.size(0)#?????????????batch_size
        output = self.conv1(input)#batch_size*10*24*24
        output = F.relu(output)#???????batch_size*10*24*24
        output = F.max_pool2d(output,2,2) #?????batch_size*10*12*12
        output = output.view(input_size,-1)#??output???batch_size*1440
        output = self.linear1(output)#???batch_size*100
        output = F.relu(output)#????????batch_size*100
        output = self.linear2(output)#???batch_size*10
        #?????????????????????????????????????????????????????????????????????lg sofmax????????
        output = F.log_softmax(output, dim=1) #????????batch_size*10
        return output

#?????
train_data = datasets.MNIST(
            root="./data/",
            train=True,
            transform=transforms.ToTensor(),
            download=True)

test_data = datasets.MNIST(
            root="./data/",
            train=False,
            transform=transforms.ToTensor(),
            download=True)

#?????
train_data_loader = torch.utils.data.DataLoader(
        dataset=train_data,
        batch_size=64,
        shuffle=True,
        drop_last=True)

test_data_loader = torch.utils.data.DataLoader(
        dataset=test_data,
        batch_size=64,
        shuffle=False,
        drop_last=False)


def train(model,train_loader,optimizer,epoch):#????????
    model.train()#?? batch normalization ? dropout 
    for data,target in  train_loader:#data???target?????
        
        optimizer.zero_grad()#????????
        output= model(data)#????
        print(output)
        loss = F.nll_loss(output, target)#????,10???????????nll_loss?target????????????????????target????????
        loss.backward()#????
        optimizer.step()#?????????????????????

def test(model,test_loader):#????
    model.eval()#??? batch normalization ? dropout 
    test_loss = 0#???????
    correct = 0#???????
    with torch.no_grad():#??????????????????
        for data,target in test_loader:
            output = model(data)#???10????????
            test_loss += F.nll_loss(output, target, reduction='sum').item() #????????
            pred = output.max(1, keepdim=True)[1] #??????????
            correct += pred.eq(target.view_as(pred)).sum().item()#??????????
    test_loss /= len(test_loader.dataset)#?????
    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))
    accuracy = correct / len(test_loader.dataset)
    return accuracy,test_loss

def main():
    model = CNN()
    optimizer = optim.Adam(model.parameters())#??????
    EPOCHS = 1#?????
    loss = []#???????????
    accuracy = []#??????????
    for epoch in range(1, EPOCHS + 1):
        print("EPOCH ",epoch,":")
        train(model,  train_data_loader, optimizer, epoch)#????????
        accuracy_tmp,loss_tmp = test(model, test_data_loader)#??????????
        accuracy.append(accuracy_tmp)
        loss.append(loss_tmp)
    #??????
    plt.plot([i+1 for i in range(len(accuracy))],accuracy,color="r")
    plt.title("Accuracy")
    plt.xlabel('epoch')
    plt.ylabel('accuracy') 
    plt.show()
    #??loss???
    plt.plot([i+1 for i in range(len(loss))],loss,color="r")
    plt.title("loss")
    plt.xlabel('epoch')
    plt.ylabel('average loss') 
    plt.show()

if __name__ == '__main__':
    main()
